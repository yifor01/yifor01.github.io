[{"title":"Deep Natural Language Processing for LinkedIn Search","path":"/2023/04/09/20230409/","content":"本文為 “Deep Natural Language Processing for LinkedIn Search” (2021.08) 的論文重點摘要 論文全文參考 width=400pxhttps://arxiv.org/abs/2108.13300 Description Goal 討論在搜尋引擎的 5+1 項 NLP 任務實務上的作法 在重視延遲問題的搜尋任務上, 如何導入 BERT 三大挑戰 延遲性：搜尋引擎最重要的問題之一。 穩定性：這邊主要是提到 DL overfit 的問題。 有效性：找出適用各種任務的最佳解法, 可能是 rule base 或 DL model。 Search system overview 大致流程為: User 輸入搜尋句 → 檢查搜尋句完整性 → 判斷搜尋意圖 → 找尋相關候選文章 → 輸出相關候選文章排序 六大搜尋任務 六大任務概覽 Query Intention Prediction Goal: 判斷 User 搜尋的目的是屬於哪一個既有標籤分類 (7類) NLP task: Text Classification Difficulty: 搜尋內容比一般文章來的短造成更嚴重的歧義問題 Example: michael dell (person names), dell engineer jobs (company) Solution (short-term) Method: 使用 TextCNN 的作法, backbone 使用了 GloVe 作為 embedding, 並結合了手工特徵(用戶行為, 文章統計, search log) Finding 不用手工特徵, 準確度掉了 0.4% LSTM 雖然準度有提升 0.2%, 但延遲率較 CNN 增加了 0.5ms CNN based query intent prediction Solution (long-term) Method: 將 backbone 換成 LiBERT (Linkedin BERT) Finding 準度較 CNN 提升了 3.28%, 但沒有比較延遲率 Query Tagging Goal: 抓出 query 中包含的 entity NLP task: NER Difficulty: entity 有嵌套以及歧異的問題 Example: research scientist (title), research (skill), scientist (title) Solution Feature: 分為 char based, word based, lexicon base Method: 使用 semi-markov conditional random f ield (SCRF) Finding: 由於 query 本身已經很短了, 還要再從中抽取 entity 導致大多 DL 模型效果不如建立辭典的方式 (lexicon) Document Ranking Goal: 給定 Query, 從一堆文章找出相關度排序 NLP task: Semantic Textual Similarity Difficulty: 延遲性 &amp; 持續有效性 Solution Method Step 1: 提取 Query 和一份文檔有多個區塊 embedding Step 2: 計算 Query embedding 和各區塊 embedding 的 cos similarity Step 3: 將 cos 特徵和手工特徵 concat 起來 Step 4: 建立 learning-to-rank layer 來計算此文檔的分數 模型架構 (不包含 learning-to-rank layer) Supplementary learning-to-rank layer Reference：Neural Ranking Models with Multiple Document Fields Sign: Ψ\\PsiΨ : matching netword ΛD\\Lambda_DΛD​ : 集成 D 文檔的各區塊 ΦQ\\Phi_QΦQ​ : query representation ΦD\\Phi_DΦD​ : doc representation ΦF\\Phi_FΦF​ : D 文檔的各個 field representation DDD 文檔的檢索分數 = Ψ(ΦQ,ΦD)\\Psi(\\Phi_Q,\\Phi_D)Ψ(ΦQ​,ΦD​) = Ψ(ΦQ,ΛD(ΦF1(F1),ΦF2(F2),...,ΦFk(Fk)))\\Psi(\\Phi_Q,\\Lambda_D(\\Phi_{F1}(F1),\\Phi_{F2}(F2),...,\\Phi_{Fk}(Fk)))Ψ(ΦQ​,ΛD​(ΦF1​(F1),ΦF2​(F2),...,ΦFk​(Fk))) ranking model 架構 Finding: 手工特徵是在個任務的實驗上屬於強力特徵。個人抱持懷疑的態度，畢竟驗證指標使用的是 NDCG, 而非衡量文章相似度作為指標。 Query Auto Completion Goal: 在 user 輸入關鍵字的時, 同時推薦可能想輸入的候選字 NLP task: Language Generation Difficulty: 延遲性 Solution Candidate Generation 對於過往出現過的關鍵詞, 可以用記憶體存取的方式直接讀取 對於未出現的關鍵詞, 採用啟發式的方式產生候選字 啟發式結果可以從三個方式去得 一定時間內, 同個 session 接續查詢的 query (水平) 同用戶輸入的兩個 query 有 co-word (垂直) 不同用戶兩個 query 同時出現 Reference: Mitra and Craswell (2015) Candidate Ranking LSTM 計算後面要接龍字的分數值 query auto completion 架構 (不包含 learning-to-rank layer) Query Suggestion Goal: 在搜尋結束時, 給予 user 下個搜尋字推薦, 類似推薦文章的想法 NLP task: Machine Translation (Seq2Seq) Difficulty: 延遲性, 穩定性 Solution seq2seq model Finding 此任務也可以用來做 query rewrite, 也許可以避免一些 user query 的冷門字 Example: software developer → software engineer BERT Pretrained Goal: 訓練基於自己 domain 的 BERT model NLP task: Eembedding Difficulty: 延遲性 Solution 收集自有的 domain data 使用輕量的 BERT 架構 (12層 → 6層), 以利加快推論速度 Conclusion 依據任務 &amp; 資料特性決定要使用甚麼樣的 model, 不一定都要套用 DL model 文中指出在 Query Tagging 和 Query Auto Completion (seen) 上 DL model 沒有 benefit 對於延遲性的建議 重新設計算法 (e.g., query auto completion) 平行計算 (e.g., query suggestion) Embedding pre-sum (e.g., document ranking) Two stage ranking (e.g., document ranking) 對於穩定性的建議 Check training data, 剔除高度相似的資料 Reuse 手工特徵, 這邊指的是避免過度依賴文字相關的結果","tags":["paper study"],"categories":["NLP"]},{"title":"Meta Segment Anything","path":"/2023/04/08/20230408/","content":"本文介紹 Meta FAIR 新出的套件 segment-anything 的實作方法 &amp; 結果 Demo 網站 https://segment-anything.com/https://segment-anything.com/ Description Goal 自動化 segment 任務 (zero shot), 並可以根據 prompt (point, box, text) 進行調整 Data SA-1B: 高達 11M 張圖片, 1.1B 個 mask 結果 (由 SAM 生成) Result 僅實驗自動化切割功能, 文字 prompt 功能未釋出 插畫自動切割 原圖 圖片來源: midjourney 產生 Segment 結果 部分星星沒有被切割出來是因為有設定 threshold 來避免切出太小的物件, 共 109 個區域 Segment 後處理 挑選最大面積的 6 個物件顯示 Wafer Map 自動切割 原圖 圖片來源: Development of High Power Green Light Emitting Diode Chips paper Segment 結果 部分文字沒有被切割出來是因為有設定 threshold 來避免切出太小的物件, 共 147 個區域 Segment 後處理 挑選最大面積的 4 個物件顯示 Model Architecture 圖片來源: segment-anything paper Practice Step 1: 模型下載 SAM model 下載 download, 放入 model 資料夾中 123mkdir modelcd modelwget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth Step 2: 讀入權重 import 相關套件 &amp; load model weight 12345678from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictorsam_checkpoint = &quot;./model/sam_vit_h_4b8939.pth&quot;model_type = &quot;vit_h&quot;device = &quot;cuda&quot;sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)sam.to(device=device) Step 3: 載入圖片 用 cv2 載入圖片並轉為 array 形式 123import cv2image = cv2.imread(&#x27;./data/test.png&#x27;)image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) Step 4: 設定模型參數 &amp; 預測 套用 mask 生成器並設定相關參數, 實作後認為 points_per_side, pred_iou_thresh 較為重要。 points_per_side 是控制採樣點的個數, 直接影響到輸出 mask 的質量 pred_iou_thresh 是輸出 mask 機率的 threshold 輸出結果包含了每個 mask 結果的面積大小, bounding box, mask 座標等等。 12345678910mask_generator = SamAutomaticMaskGenerator( model=sam, points_per_side=32, pred_iou_thresh=0.9, stability_score_thresh=0.92, crop_n_layers=1, crop_n_points_downscale_factor=2, min_mask_region_area=100, # Requires open-cv to run post-processing)masks2 = mask_generator.generate(image) Step 5: 顯示分割結果 依照 mask 面積大小排序對原圖進行 mask 著色 12345678910111213141516171819202122import numpy as npimport matplotlib.pyplot as pltdef show_anns(anns): if len(anns) == 0: return sorted_anns = sorted(anns, key=(lambda x: x[&#x27;area&#x27;]), reverse=True) ax = plt.gca() ax.set_autoscale_on(False) for ann in sorted_anns: m = ann[&#x27;segmentation&#x27;] img = np.ones((m.shape[0], m.shape[1], 3)) color_mask = np.random.random((1, 3)).tolist()[0] for i in range(3): img[:,:,i] = color_mask[i] ax.imshow(np.dstack((img, m*0.35)))plt.figure(figsize=(15,15))plt.imshow(image)show_anns(masks2)plt.axis(&#x27;off&#x27;)plt.show() Step 6: 去背結果 建立一個 mask matrix 在乘上原本圖片 matrix 後, 即可得到去背的圖片 123456789101112final_mask = np.zeros(image.shape[:2],dtype=bool)for i in range(len(masks2)): final_mask +=masks2[i][&#x27;segmentation&#x27;]mask_image = image.copy()for i in range(3): mask_image[:,:,i] *=final_maskplt.figure(figsize=(15,15))plt.imshow(mask_image)plt.axis(&#x27;off&#x27;)plt.show()","tags":["practice","Meta"],"categories":["CV"]},{"title":"LLaMA 套件踩坑","path":"/2023/04/07/20230407/","content":"本文介紹 LLaMA 系列套件安裝踩過的一些坑和解法 bitsandbytes load_in_8bit error Root Cause: 系統預設的 bitsandbytes_cpu.so 與 cuda 版本不同 Action: 將自己的版本的 bitsandbytes_cuda&lt;cuda版號&gt;.so 替換為 bitsandbytes_cpu.so step1: mv lib/python3.9/site-packages/bitsandbytes/lib/bitsandbytes_cpu.so lib/python3.9/site-packages/bitsandbytes/lib/bitsandbytes_cuda114.so step2: cp lib/python3.9/site-packages/bitsandbytes/lib/bitsandbytes_cuda117.so lib/python3.9/site-packages/bitsandbytes/lib/bitsandbytes_cpu.so transformer 目前僅有 transformers-main 的版本支援 LlamaTokenizer 系列的子套件 12pip uninstall transformerspip install git+https://github.com/huggingface/transformers.git pytorch docker LLaMA 最低 python 環境要求為 python 3.8, 如何的套用現成 Docker Image? 以下是 pytorch 在 dockerhub 各版本對應到的 python 環境:","tags":["practice"],"categories":["NLP"]},{"path":"/about/index.html","content":"About Me 一位誤入動物園的 NLP 新手, 專長是販賣各種動物周邊 (NLP 建模) 和 Boost 系列跑鞋 (ML 建模)。 Job Experiment Now AI Engineer at TSMCDefect Report 自動化知識抽取模組2022 年 6 月 Machine Learning Engineer at Invos既有產品品類多標籤分類模型新品牌、新品類自動化發現模型2022 年 3 月 Senior Data Scientist at Wisers專利發表新詞發現算法⾏業專業詞挖掘算法商用 API 開發關鍵詞抽取模組 (包含新詞、熱詞、領域關鍵詞)購買意願模型 (包含中文、廣東話)評論分類模型英文情感分析模型技術研發自動化 POC tagging (支援 8個API, 14個客製化抽取算法)運動品牌知識圖譜優化特定實體的購買意願模型2020 年 2 月 Data Scientist at GSSCRM Insight 產品維護 &amp; 新功能開發行銷最佳化 BI公司產品關鍵字廣告成效分析2019 年 7 月"},{"path":"/notes/index.html","content":"項目 競賽 2022 玉山人工智慧挑戰賽 - 你說可疑不可疑？ 疑似洗錢交易預測 https://github.com/ansonchang/esun2022https://github.com/ansonchang/esun2022 2021 中鋼人工智慧挑戰賽 - 字元辨識 https://github.com/yifor01/CSC-OCR-competitionhttps://github.com/yifor01/CSC-OCR-competition 2020 I'm the Best Coder Challenge! 2020 https://github.com/yifor01/High-Value-Customer-Forecast-Competitionhttps://github.com/yifor01/High-Value-Customer-Forecast-Competition"}]